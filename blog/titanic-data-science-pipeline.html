<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Titanic Dataset Analysis: A Complete Data Science Pipeline - Dr. Sinan</title>
    <link rel="stylesheet" href="../styles/main.css">
    <link rel="stylesheet" href="../styles/blog.css">
    <link rel="stylesheet" href="../styles/post.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-logo">
                <a href="../index.html">Dr. Sinan</a>
            </div>
            <ul class="nav-menu">
                <li class="nav-item">
                    <a href="../index.html#home" class="nav-link">Home</a>
                </li>
                <li class="nav-item">
                    <a href="../index.html#about" class="nav-link">About</a>
                </li>
                <li class="nav-item">
                    <a href="../index.html#research" class="nav-link">Research</a>
                </li>
                <li class="nav-item">
                    <a href="../index.html#projects" class="nav-link">Projects</a>
                </li>
                <li class="nav-item">
                    <a href="../blog.html" class="nav-link">Blog</a>
                </li>
                <li class="nav-item">
                    <a href="../index.html#contact" class="nav-link">Contact</a>
                </li>
            </ul>
            <div class="hamburger">
                <span class="bar"></span>
                <span class="bar"></span>
                <span class="bar"></span>
            </div>
        </div>
    </nav>

    <!-- Post Header -->
    <header class="post-header">
        <div class="container">
            <div class="breadcrumb">
                <a href="../blog.html"><i class="fas fa-arrow-left"></i> Back to Blog</a>
            </div>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> July 23, 2025</span>
                <span><i class="far fa-clock"></i> 18 min read</span>
                <span><i class="fas fa-tag"></i> Data Science</span>
                <span><i class="fas fa-tag"></i> Machine Learning</span>
                <span><i class="fas fa-tag"></i> Classification</span>
            </div>
            <h1 class="post-title">Titanic Dataset Analysis: A Complete Data Science Pipeline</h1>
            <p class="post-subtitle">From data understanding to deployment - A comprehensive 5-phase approach to machine learning with the classic Titanic survival prediction challenge, including feature engineering, model comparison, and Tkinter application deployment</p>
            <div class="author-info">
                <img src="../assets/images/profile.jpg" alt="Dr. Sinan" class="author-avatar">
                <div class="author-details">
                    <h3>Dr. Sinan</h3>
                    <p>Research Scientist & ML Engineer</p>
                </div>
            </div>
        </div>
    </header>

    <!-- Post Content with Sidebar -->
    <div class="post-content">
        <div class="blog-post-layout">
            <!-- Table of Contents Sidebar -->
            <aside class="table-of-contents">
                <h4>Table of Contents</h4>
                <ul>
                    <li><a href="#introduction">Introduction</a></li>
                    <li><a href="#project-overview">Project Overview</a></li>
                    <li><a href="#phase1-understanding">Phase 1: Data Understanding</a></li>
                    <li class="h3"><a href="#data-loading">Data Loading & Preview</a></li>
                    <li class="h3"><a href="#data-exploration">Initial Exploration</a></li>
                    <li><a href="#phase2-cleaning">Phase 2: Data Cleaning</a></li>
                    <li class="h3"><a href="#missing-values">Handling Missing Values</a></li>
                    <li class="h3"><a href="#outliers">Outlier Detection</a></li>
                    <li class="h3"><a href="#duplicates">Duplicate Removal</a></li>
                    <li><a href="#phase3-engineering">Phase 3: Feature Engineering</a></li>
                    <li class="h3"><a href="#normalization">Data Normalization</a></li>
                    <li class="h3"><a href="#encoding">Categorical Encoding</a></li>
                    <li class="h3"><a href="#feature-selection">Feature Selection</a></li>
                    <li><a href="#phase4-splitting">Phase 4: Data Splitting</a></li>
                    <li class="h3"><a href="#train-test-split">Train-Test Split</a></li>
                    <li class="h3"><a href="#stratified-sampling">Stratified Sampling</a></li>
                    <li><a href="#phase5-modeling">Phase 5: Modeling & Evaluation</a></li>
                    <li class="h3"><a href="#baseline-model">Baseline Model</a></li>
                    <li class="h3"><a href="#model-comparison">Model Comparison</a></li>
                    <li class="h3"><a href="#hyperparameter-tuning">Hyperparameter Tuning</a></li>
                    <li class="h3"><a href="#feature-importance">Feature Importance</a></li>
                    <li><a href="#deployment">Deployment</a></li>
                    <li><a href="#conclusion">Conclusion</a></li>
                    <li><a href="#resources">Resources</a></li>
                </ul>
            </aside>

            <!-- Main Content -->
            <main class="blog-post-main">
                <div class="post-article">
                    <section id="introduction">
                        <h2>Introduction</h2>
                        <p>The Titanic dataset represents one of the most famous datasets in data science education and competition. This tragic maritime disaster from 1912 provides a rich source of data for understanding machine learning fundamentals while tackling a real-world classification problem: predicting passenger survival based on various demographic and social factors.</p>
                        
                        <p>In this comprehensive tutorial, we'll walk through a complete data science pipeline from initial data understanding to model deployment. This project demonstrates best practices in data science methodology, following a systematic 5-phase approach that can be applied to any machine learning project.</p>

                        <div class="info-box">
                            <h4><i class="fab fa-github"></i> Project Repository</h4>
                            <p>The complete data science pipeline with all phases is available on GitHub:</p>
                            <a href="https://github.com/sinanLab/titanic_dataset" target="_blank" class="github-link">
                                <i class="fab fa-github"></i> View GitHub Repository
                            </a>
                        </div>
                    </section>

                    <section id="project-overview">
                        <h2>Project Overview</h2>
                        <p>Our Titanic survival analysis follows a structured 5-phase methodology that ensures comprehensive coverage of the data science lifecycle:</p>
                        
                        <div class="result-box">
                            <h4>Project Phases:</h4>
                            <ol>
                                <li><strong>Phase 1: Data Understanding</strong> - Initial exploration and data profiling</li>
                                <li><strong>Phase 2: Data Cleaning</strong> - Handling missing values, outliers, and duplicates</li>
                                <li><strong>Phase 3: Feature Engineering</strong> - Normalization, encoding, and feature selection</li>
                                <li><strong>Phase 4: Data Splitting</strong> - Train-test splits with stratification</li>
                                <li><strong>Phase 5: Modeling & Evaluation</strong> - Model comparison, tuning, and analysis</li>
                            </ol>
                        </div>

                        <div class="visualization-section">
                            <h4>Dataset Context:</h4>
                            <p>The RMS Titanic sank on April 15, 1912, during its maiden voyage. Our analysis aims to understand which factors contributed most to passenger survival, using features such as:</p>
                            <ul>
                                <li><strong>Demographics:</strong> Age, gender, family relationships</li>
                                <li><strong>Socioeconomic:</strong> Passenger class, fare paid</li>
                                <li><strong>Logistics:</strong> Cabin location, embarkation port</li>
                                <li><strong>Family:</strong> Number of siblings/spouses and parents/children aboard</li>
                            </ul>
                        </div>
                    </section>

                    <section id="phase1-understanding">
                        <h2>Phase 1: Data Understanding</h2>
                        
                        <h3 id="data-loading">Data Loading & Initial Preview</h3>
                        <p>We begin our analysis by loading the Titanic dataset and conducting initial exploration:</p>

<pre><code class="language-python">import seaborn as sns
import pandas as pd

# Load Titanic dataset from seaborn
df = sns.load_dataset("titanic")

# First glimpse of the data
print("First 5 rows:")
print(df.head())

# Dataset dimensions
print(f"\nDataset shape: {df.shape}")
print(f"Total samples: {df.shape[0]}")
print(f"Total features: {df.shape[1]}")

# Save initial dataset
df.to_csv("data/phase_1_titanic_dataset.csv", index=False)</code></pre>

                        <h3 id="data-exploration">Initial Data Exploration</h3>
                        <p>Understanding the structure and characteristics of our dataset:</p>

<pre><code class="language-python"># Comprehensive dataset information
print("Dataset Info:")
print(df.info())

# Statistical summary for numerical columns
print("\nNumerical Statistics:")
print(df.describe())

# Data types examination
print("\nData Types:")
print(df.dtypes)

# Check for missing values
print("\nMissing Values Count:")
print(df.isnull().sum())

# Unique values in categorical columns
categorical_cols = df.select_dtypes(include=['object', 'category']).columns
for col in categorical_cols:
    print(f"\n{col}: {df[col].unique()}")</code></pre>

                        <div class="result-box">
                            <h4>Initial Data Insights:</h4>
                            <ul>
                                <li><strong>Dataset Size:</strong> 891 passengers with 15 features</li>
                                <li><strong>Target Variable:</strong> 'survived' (0 = No, 1 = Yes)</li>
                                <li><strong>Missing Data:</strong> Age (~20%), Deck (~77%), Embark_town (~0.2%)</li>
                                <li><strong>Data Types:</strong> Mix of numerical and categorical features</li>
                                <li><strong>Class Distribution:</strong> More non-survivors than survivors</li>
                            </ul>
                        </div>
                    </section>

                    <section id="phase2-cleaning">
                        <h2>Phase 2: Data Cleaning</h2>
                        
                        <h3 id="missing-values">Handling Missing Values</h3>
                        <p>We implement different strategies for different types of missing data:</p>

<pre><code class="language-python">import pandas as pd

# Load data from previous phase
df = pd.read_csv("data/phase_1_titanic_dataset.csv")

# Check current missing values
print("Missing values before cleaning:")
print(df.isnull().sum())

# 🔹 Numerical Columns - Use median for age
df['age'] = df['age'].fillna(df['age'].median())

# 🔹 Categorical Columns - Use mode for most common value
df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])
df['embark_town'] = df['embark_town'].fillna(df['embark_town'].mode()[0])

# 🔹 Deck column - Use 'Unknown' for missing values
df['deck'] = df['deck'].fillna('Unknown')

print("\nMissing values after cleaning:")
print(df.isnull().sum())</code></pre>

                        <h3 id="outliers">Outlier Detection and Removal</h3>
                        <p>Using the Interquartile Range (IQR) method to handle fare outliers:</p>

<pre><code class="language-python"># Outlier detection using IQR method for fare
Q1 = df['fare'].quantile(0.25)
Q3 = df['fare'].quantile(0.75)
IQR = Q3 - Q1

# Calculate outlier boundaries
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

print(f"Fare outlier bounds: [{lower_bound:.2f}, {upper_bound:.2f}]")

# Filter out outliers
original_size = len(df)
df = df[(df['fare'] >= lower_bound) & (df['fare'] <= upper_bound)]
new_size = len(df)

print(f"Removed {original_size - new_size} outliers ({((original_size - new_size)/original_size)*100:.1f}%)")</code></pre>

                        <h3 id="duplicates">Duplicate Removal</h3>
                        <p>Ensuring data quality by removing duplicate records:</p>

<pre><code class="language-python"># Check for and remove duplicates
duplicates_before = df.duplicated().sum()
df = df.drop_duplicates()
duplicates_after = df.duplicated().sum()

print(f"Duplicates removed: {duplicates_before}")
print(f"Final dataset size: {len(df)} samples")

# Convert target to categorical
df['survived'] = df['survived'].astype('category')

# Save cleaned dataset
df.to_csv("data/phase_2_titanic_dataset.csv", index=False)</code></pre>

                        <div class="result-box">
                            <h4>Data Cleaning Results:</h4>
                            <ul>
                                <li><strong>Missing Values:</strong> All handled with appropriate strategies</li>
                                <li><strong>Age:</strong> Filled with median (28.0 years)</li>
                                <li><strong>Embarked:</strong> Filled with mode ('S' - Southampton)</li>
                                <li><strong>Deck:</strong> Missing values labeled as 'Unknown'</li>
                                <li><strong>Outliers:</strong> Extreme fare values removed using IQR method</li>
                                <li><strong>Duplicates:</strong> Any duplicate rows eliminated</li>
                            </ul>
                        </div>
                    </section>

                    <section id="phase3-engineering">
                        <h2>Phase 3: Feature Engineering</h2>
                        
                        <h3 id="normalization">Data Normalization</h3>
                        <p>Standardizing numerical features to improve model performance:</p>

<pre><code class="language-python">import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Load cleaned data
df = pd.read_csv("data/phase_2_titanic_dataset.csv")

# Min-Max Normalization (0-1 scaling)
# Formula: x' = (x - min(x)) / (max(x) - min(x))
scaler = MinMaxScaler()

# Apply normalization to numerical features
df['age_norm'] = scaler.fit_transform(df[['age']])
df['fare_norm'] = scaler.fit_transform(df[['fare']])

print("Normalization completed:")
print(f"Age range: [{df['age_norm'].min():.3f}, {df['age_norm'].max():.3f}]")
print(f"Fare range: [{df['fare_norm'].min():.3f}, {df['fare_norm'].max():.3f}]")</code></pre>

                        <h3 id="encoding">Categorical Encoding</h3>
                        <p>Converting categorical variables to numerical format for machine learning:</p>

<pre><code class="language-python">from sklearn.preprocessing import LabelEncoder

# 🔹 1. Label Encoding for binary features
le = LabelEncoder()
df['sex_encoded'] = le.fit_transform(df['sex'])  # male=1, female=0
print(f"Sex encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}")

# 🔹 2. One-Hot Encoding for multi-category features
# This creates binary columns for each category
df = pd.get_dummies(df, columns=['embarked', 'class'], drop_first=True)

print("\nNew columns after one-hot encoding:")
print([col for col in df.columns if 'embarked_' in col or 'class_' in col])</code></pre>

                        <h3 id="feature-selection">Feature Selection</h3>
                        <p>Removing redundant and non-predictive features to simplify the model:</p>

<pre><code class="language-python"># Remove original columns that have been encoded or are not useful for prediction
columns_to_drop = [
    'sex',          # Replaced by sex_encoded
    'age',          # Replaced by age_norm  
    'fare',         # Replaced by fare_norm
    'deck',         # Too many missing values
    'embark_town',  # Redundant with embarked
    'who',          # Redundant with sex and age
    'alive',        # Same as survived
    'adult_male'    # Derivable from sex and age
]

df = df.drop(columns=columns_to_drop)

print("Final feature set:")
print(list(df.columns))

# Save feature-engineered dataset
df.to_csv("data/phase_3_titanic_dataset.csv", index=False)</code></pre>

                        <div class="visualization-section">
                            <h4>Feature Engineering Summary:</h4>
                            <ul>
                                <li><strong>Normalization:</strong> Age and fare scaled to [0,1] range</li>
                                <li><strong>Label Encoding:</strong> Binary sex variable converted to 0/1</li>
                                <li><strong>One-Hot Encoding:</strong> Embarked and class expanded to binary features</li>
                                <li><strong>Feature Reduction:</strong> Removed 8 redundant/non-predictive columns</li>
                                <li><strong>Final Features:</strong> 11 engineered features for modeling</li>
                            </ul>
                        </div>
                    </section>

                    <section id="phase4-splitting">
                        <h2>Phase 4: Data Splitting</h2>
                        
                        <h3 id="train-test-split">Train-Test Split with Stratification</h3>
                        <p>Properly splitting the data while maintaining class balance:</p>

<pre><code class="language-python">import pandas as pd
from sklearn.model_selection import train_test_split

# Load feature-engineered data
df = pd.read_csv("data/phase_3_titanic_dataset.csv")

# Separate features (X) and target (y)
X = df.drop(columns=['survived'])  # All features except target
y = df['survived']                 # Target variable

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")
print(f"Target distribution:\n{y.value_counts(normalize=True)}")

# Stratified train-test split to maintain class balance
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,        # 20% for testing
    random_state=42,      # For reproducibility
    stratify=y            # Maintain class distribution in both sets
)

print(f"\nTraining set size: {X_train.shape[0]}")
print(f"Test set size: {X_test.shape[0]}")
print(f"Training target distribution:\n{y_train.value_counts(normalize=True)}")
print(f"Test target distribution:\n{y_test.value_counts(normalize=True)}")</code></pre>

                        <div class="result-box">
                            <h4>Data Splitting Results:</h4>
                            <ul>
                                <li><strong>Training Set:</strong> 80% of data for model training</li>
                                <li><strong>Test Set:</strong> 20% of data for final evaluation</li>
                                <li><strong>Stratification:</strong> Maintains survival rate balance in both sets</li>
                                <li><strong>Feature Count:</strong> 11 engineered features</li>
                                <li><strong>Random State:</strong> Set for reproducible results</li>
                            </ul>
                        </div>
                    </section>

                    <section id="phase5-modeling">
                        <h2>Phase 5: Modeling & Evaluation</h2>
                        
                        <h3 id="baseline-model">Baseline Model Development</h3>
                        <p>Starting with a simple Logistic Regression as our baseline:</p>

<pre><code class="language-python">from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Create and train baseline model
baseline_model = LogisticRegression(max_iter=1000)
baseline_model.fit(X_train, y_train)

# Make predictions
y_pred_baseline = baseline_model.predict(X_test)

# Evaluate baseline performance
baseline_accuracy = accuracy_score(y_test, y_pred_baseline)
print(f"Baseline Accuracy: {baseline_accuracy:.4f}")
print("\nBaseline Classification Report:")
print(classification_report(y_test, y_pred_baseline))</code></pre>

                        <h3 id="model-comparison">Comprehensive Model Comparison</h3>
                        <p>Evaluating multiple algorithms to find the best performer:</p>

<pre><code class="language-python">from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Define models for comparison
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Support Vector Machine": SVC(random_state=42)
}

# Store results for comparison
results = []

# Train and evaluate each model
for name, model in models.items():
    # Train model
    model.fit(X_train, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test)
    
    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, output_dict=True)
    
    # Store results
    results.append({
        'Model': name,
        'Accuracy': round(accuracy, 4),
        'Precision_0': round(report['0']['precision'], 4),
        'Recall_0': round(report['0']['recall'], 4),
        'Precision_1': round(report['1']['precision'], 4),
        'Recall_1': round(report['1']['recall'], 4),
        'F1_1': round(report['1']['f1-score'], 4)
    })
    
    print(f"\n📌 {name}")
    print(f"Accuracy: {accuracy:.4f}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

# Create comparison DataFrame
import pandas as pd
results_df = pd.DataFrame(results)
results_df = results_df.sort_values('F1_1', ascending=False)
print("\n📊 Model Comparison Summary:")
print(results_df)</code></pre>

                        <div class="result-box">
                            <h4>Model Performance Summary:</h4>
                            <table>
                                <thead>
                                    <tr>
                                        <th>Model</th>
                                        <th>Accuracy</th>
                                        <th>Precision (Survivors)</th>
                                        <th>Recall (Survivors)</th>
                                        <th>F1-Score</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>K-Nearest Neighbors</strong></td>
                                        <td>0.7941</td>
                                        <td>0.75</td>
                                        <td>0.60</td>
                                        <td><strong>0.68</strong></td>
                                    </tr>
                                    <tr>
                                        <td>Logistic Regression</td>
                                        <td>0.7721</td>
                                        <td>0.72</td>
                                        <td>0.56</td>
                                        <td>0.64</td>
                                    </tr>
                                    <tr>
                                        <td>Random Forest</td>
                                        <td>0.7426</td>
                                        <td>0.69</td>
                                        <td>0.64</td>
                                        <td>0.65</td>
                                    </tr>
                                    <tr>
                                        <td>Decision Tree</td>
                                        <td>0.7426</td>
                                        <td>0.68</td>
                                        <td>0.62</td>
                                        <td>0.64</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <h3 id="feature-importance">Feature Importance Analysis</h3>
                        <p>Understanding which features contribute most to survival prediction:</p>

<pre><code class="language-python">import matplotlib.pyplot as plt
import numpy as np

# Use Random Forest for feature importance analysis
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# Get feature importances
feature_importance = rf_model.feature_importances_
feature_names = X_train.columns

# Create feature importance DataFrame
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importance
}).sort_values('Importance', ascending=False)

print("Feature Importance Ranking:")
print(importance_df)

# Visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(range(len(importance_df)), importance_df['Importance'])
plt.yticks(range(len(importance_df)), importance_df['Feature'])
plt.xlabel('Feature Importance')
plt.title('Random Forest Feature Importance for Titanic Survival Prediction')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig("results/feature_importance.png", dpi=300, bbox_inches='tight')
plt.show()</code></pre>

                        <div class="key-insights">
                            <h4>🎯 Key Feature Insights:</h4>
                            <ul>
                                <li><strong>Sex (Gender):</strong> Most important predictor - "women and children first" policy</li>
                                <li><strong>Fare:</strong> Higher fares indicated better accommodations and survival chances</li>
                                <li><strong>Age:</strong> Younger passengers had higher survival rates</li>
                                <li><strong>Passenger Class:</strong> First-class passengers had better survival odds</li>
                                <li><strong>Family Size:</strong> Being alone vs. traveling with family affected survival</li>
                            </ul>
                        </div>
                    </section>

                    <section id="deployment">
                        <h2>Model Deployment</h2>
                        <p>Creating a user-friendly application for real-world use of our trained model:</p>

<pre><code class="language-python">import tkinter as tk
from tkinter import messagebox
import joblib
import numpy as np

# Save the best model (Random Forest)
import joblib
joblib.dump(rf_model, "deployment/random_forest_model.pkl")

class TitanicApp:
    def __init__(self, root):
        self.root = root
        self.root.title("Titanic Survival Predictor")
        self.root.geometry("400x600")
        
        # Load the trained model
        self.model = joblib.load("deployment/random_forest_model.pkl")
        
        # Create input fields for all features
        self.entries = {}
        fields = [
            'pclass', 'sibsp', 'parch', 'alone', 'age_norm', 'fare_norm',
            'sex_encoded', 'embarked_Q', 'embarked_S', 'class_Second', 'class_Third'
        ]
        
        for field in fields:
            label = tk.Label(root, text=field.replace('_', ' ').title())
            label.pack(pady=2)
            entry = tk.Entry(root)
            entry.pack(pady=2)
            self.entries[field] = entry
        
        # Prediction button
        predict_btn = tk.Button(root, text="Predict Survival", 
                               command=self.predict, bg='#2563eb', fg='white')
        predict_btn.pack(pady=10)
        
        # Result display
        self.result_label = tk.Label(root, text="", font=("Arial", 14))
        self.result_label.pack(pady=10)
    
    def predict(self):
        try:
            # Collect input values
            input_values = []
            for field in self.entries:
                value = float(self.entries[field].get())
                input_values.append(value)
            
            # Make prediction
            X_input = np.array([input_values])
            prediction = self.model.predict(X_input)[0]
            probability = self.model.predict_proba(X_input)[0]
            
            # Display result
            if prediction == 1:
                result = f"SURVIVED\nConfidence: {probability[1]:.2%}"
                self.result_label.config(text=result, fg='green')
            else:
                result = f"DID NOT SURVIVE\nConfidence: {probability[0]:.2%}"
                self.result_label.config(text=result, fg='red')
                
        except ValueError:
            messagebox.showerror("Error", "Please enter valid numbers for all fields")

# Run the application
if __name__ == "__main__":
    root = tk.Tk()
    app = TitanicApp(root)
    root.mainloop()</code></pre>

                        <div class="info-box">
                            <h4><i class="fas fa-desktop"></i> Desktop Application Features</h4>
                            <p>The Tkinter application provides an intuitive interface for survival prediction:</p>
                            <ul>
                                <li><strong>User-Friendly Interface:</strong> Simple form-based input for all features</li>
                                <li><strong>Real-Time Prediction:</strong> Instant results with confidence scores</li>
                                <li><strong>Model Integration:</strong> Uses the best-performing Random Forest model</li>
                                <li><strong>Error Handling:</strong> Validates input and provides helpful error messages</li>
                                <li><strong>Visual Feedback:</strong> Color-coded results (green=survived, red=not survived)</li>
                            </ul>
                        </div>
                    </section>

                    <section id="conclusion">
                        <h2>Conclusion</h2>
                        <p>This comprehensive Titanic dataset analysis demonstrates a complete data science workflow that can be applied to any classification problem. Through our systematic 5-phase approach, we've created a robust predictive model that achieves approximately 79% accuracy in predicting passenger survival.</p>
                        
                        <div class="key-insights">
                            <h4>🎯 Key Project Outcomes:</h4>
                            <ul>
                                <li><strong>Methodology:</strong> Structured 5-phase approach ensures comprehensive analysis</li>
                                <li><strong>Data Quality:</strong> Systematic cleaning and feature engineering improved model performance</li>
                                <li><strong>Model Selection:</strong> K-Nearest Neighbors emerged as the best performer</li>
                                <li><strong>Feature Insights:</strong> Gender, fare, and age were the most predictive factors</li>
                                <li><strong>Deployment:</strong> Created a user-friendly application for practical use</li>
                                <li><strong>Reproducibility:</strong> All phases documented with clear, executable code</li>
                            </ul>
                        </div>

                        <div class="clinical-implications">
                            <h4>📊 Historical Insights:</h4>
                            <p>Our analysis reveals important social and logistical factors that influenced survival on the Titanic:</p>
                            <ul>
                                <li><strong>Social Hierarchy:</strong> Passenger class significantly affected survival chances</li>
                                <li><strong>Demographics:</strong> Women and children had priority in lifeboats</li>
                                <li><strong>Economic Factors:</strong> Higher fare correlated with better survival odds</li>
                                <li><strong>Family Dynamics:</strong> Family size impacted individual survival strategies</li>
                                <li><strong>Emergency Procedures:</strong> Embarkation port suggested different response protocols</li>
                            </ul>
                        </div>

                        <div class="visualization-section">
                            <h4>🚀 Advanced Extensions:</h4>
                            <p>This foundational project can be extended in numerous ways:</p>
                            <ul>
                                <li><strong>Advanced Models:</strong> Neural networks, ensemble methods, or boosting algorithms</li>
                                <li><strong>Feature Engineering:</strong> Creating interaction terms or polynomial features</li>
                                <li><strong>Cross-Validation:</strong> More robust model evaluation with k-fold CV</li>
                                <li><strong>Hyperparameter Optimization:</strong> Grid search or Bayesian optimization</li>
                                <li><strong>Web Deployment:</strong> Flask/Django web application or cloud deployment</li>
                                <li><strong>Interpretability:</strong> LIME or SHAP for model explanation</li>
                            </ul>
                        </div>
                    </section>

                    <section id="resources">
                        <h2>Resources & Further Reading</h2>
                        <div class="resources-grid">
                            <div class="resource-card">
                                <h4><i class="fab fa-github"></i> Complete Project</h4>
                                <p>All 5 phases with Jupyter notebooks and deployment code</p>
                                <a href="https://github.com/sinanLab/titanic_dataset" target="_blank">View on GitHub</a>
                            </div>
                            <div class="resource-card">
                                <h4><i class="fas fa-database"></i> Titanic Dataset</h4>
                                <p>Original dataset information and historical context</p>
                                <a href="https://www.kaggle.com/c/titanic" target="_blank">Kaggle Competition</a>
                            </div>
                            <div class="resource-card">
                                <h4><i class="fas fa-book"></i> Scikit-learn</h4>
                                <p>Machine learning library documentation and tutorials</p>
                                <a href="https://scikit-learn.org/stable/" target="_blank">Read Docs</a>
                            </div>
                            <div class="resource-card">
                                <h4><i class="fas fa-chart-line"></i> Data Science Process</h4>
                                <p>CRISP-DM methodology for data science projects</p>
                                <a href="https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining" target="_blank">Learn More</a>
                            </div>
                        </div>
                    </section>

                    <!-- Tags and Sharing -->
                    <div class="post-footer">
                        <div class="post-tags">
                            <h4>Tags:</h4>
                            <span class="tag">Data Science</span>
                            <span class="tag">Machine Learning</span>
                            <span class="tag">Classification</span>
                            <span class="tag">Python</span>
                            <span class="tag">Pandas</span>
                            <span class="tag">Scikit-learn</span>
                            <span class="tag">Feature Engineering</span>
                            <span class="tag">Tkinter</span>
                        </div>
                        
                        <div class="share-buttons">
                            <h4>Share this article:</h4>
                            <a href="#" class="share-btn twitter"><i class="fab fa-twitter"></i> Twitter</a>
                            <a href="#" class="share-btn linkedin"><i class="fab fa-linkedin"></i> LinkedIn</a>
                            <a href="#" class="share-btn facebook"><i class="fab fa-facebook"></i> Facebook</a>
                        </div>
                    </div>

                    <!-- Author Bio -->
                    <div class="author-bio">
                        <img src="../assets/images/profile.jpg" alt="Dr. Sinan" class="bio-avatar">
                        <div class="bio-content">
                            <h3>About Dr. Sinan</h3>
                            <p>Dr. Sinan is a Research Scientist and Machine Learning Engineer with extensive experience in data science methodology and predictive modeling. He specializes in creating end-to-end machine learning solutions and educational content that bridges theory and practice.</p>
                            <div class="bio-links">
                                <a href="../index.html#contact"><i class="fas fa-envelope"></i> Contact</a>
                                <a href="#"><i class="fab fa-linkedin"></i> LinkedIn</a>
                                <a href="#"><i class="fab fa-github"></i> GitHub</a>
                            </div>
                        </div>
                    </div>

                    <!-- Related Posts CTA -->
                    <div class="related-posts-cta">
                        <h3>Explore More Data Science Projects</h3>
                        <p>Discover our other machine learning and data analysis tutorials.</p>
                        <a href="../blog.html" class="cta-button">
                            <i class="fas fa-arrow-right"></i> Browse All Posts
                        </a>
                    </div>
                </div>
            </main>
        </div>
    </div>

    <!-- Scripts -->
    <script src="../scripts/main.js"></script>
    <script src="../scripts/blog.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js"></script>
</body>
</html>
